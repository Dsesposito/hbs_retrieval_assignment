{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b781e47-2986-4c1d-b1f6-28361312aa93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens==3.0.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: certifi==2025.1.31 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: comm==0.2.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.2.2)\n",
      "Requirement already satisfied: contourpy==1.3.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: cycler==0.12.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: debugpy==1.8.13 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.8.13)\n",
      "Requirement already satisfied: decorator==5.2.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup==1.2.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.2.2)\n",
      "Requirement already satisfied: executing==2.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (2.2.0)\n",
      "Requirement already satisfied: filelock==3.18.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (3.18.0)\n",
      "Requirement already satisfied: fonttools==4.56.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (4.56.0)\n",
      "Requirement already satisfied: fsspec==2025.3.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub==0.29.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (0.29.3)\n",
      "Requirement already satisfied: idna==3.10 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (3.10)\n",
      "Requirement already satisfied: importlib_metadata==8.6.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (8.6.1)\n",
      "Requirement already satisfied: importlib_resources==6.5.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (6.5.2)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.18.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (8.18.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (0.19.2)\n",
      "Requirement already satisfied: Jinja2==3.1.6 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (3.1.6)\n",
      "Requirement already satisfied: joblib==1.4.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (1.4.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (5.7.2)\n",
      "Requirement already satisfied: kiwisolver==1.4.7 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (1.4.7)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib==3.9.4 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (3.9.4)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (0.1.7)\n",
      "Requirement already satisfied: mpmath==1.3.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.2.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (3.2.1)\n",
      "Requirement already satisfied: numpy==2.0.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 32)) (2.0.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 33)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 36)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 37)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 38)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 39)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 40)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 41)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 42)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 43)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 44)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 45)) (12.4.127)\n",
      "Requirement already satisfied: packaging==24.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 46)) (24.2)\n",
      "Requirement already satisfied: pandas==2.2.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 47)) (2.2.3)\n",
      "Requirement already satisfied: parso==0.8.4 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 48)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 49)) (4.9.0)\n",
      "Requirement already satisfied: pillow==11.1.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 50)) (11.1.0)\n",
      "Requirement already satisfied: platformdirs==4.3.7 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 51)) (4.3.7)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.50 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 52)) (3.0.50)\n",
      "Requirement already satisfied: psutil==7.0.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 53)) (7.0.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 54)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 55)) (0.2.3)\n",
      "Requirement already satisfied: Pygments==2.19.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 56)) (2.19.1)\n",
      "Requirement already satisfied: pyparsing==3.2.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 57)) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 58)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2025.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 59)) (2025.1)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 60)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.3.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 61)) (26.3.0)\n",
      "Requirement already satisfied: regex==2024.11.6 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 62)) (2024.11.6)\n",
      "Requirement already satisfied: requests==2.32.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 63)) (2.32.3)\n",
      "Requirement already satisfied: safetensors==0.5.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 64)) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 65)) (1.6.1)\n",
      "Requirement already satisfied: scipy==1.13.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 66)) (1.13.1)\n",
      "Requirement already satisfied: seaborn==0.13.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 67)) (0.13.2)\n",
      "Requirement already satisfied: sentence-transformers==3.4.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 68)) (3.4.1)\n",
      "Requirement already satisfied: six==1.17.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 69)) (1.17.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 70)) (0.6.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 71)) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl==3.6.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 72)) (3.6.0)\n",
      "Requirement already satisfied: tokenizers==0.21.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 73)) (0.21.1)\n",
      "Requirement already satisfied: torch==2.6.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 74)) (2.6.0)\n",
      "Requirement already satisfied: tornado==6.4.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 75)) (6.4.2)\n",
      "Requirement already satisfied: tqdm==4.67.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 76)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 77)) (5.14.3)\n",
      "Requirement already satisfied: transformers==4.50.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 78)) (4.50.0)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 79)) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 80)) (4.12.2)\n",
      "Requirement already satisfied: tzdata==2025.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 81)) (2025.2)\n",
      "Requirement already satisfied: urllib3==2.3.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 82)) (2.3.0)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 83)) (0.2.13)\n",
      "Requirement already satisfied: zipp==3.21.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 84)) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Python 3.9 required\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e43e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desposito/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List, Literal\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2384d13-941a-4c49-b903-68c5cf55ac15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'WANDS' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "#clone the git repo that contains the data and additional information about the dataset\n",
    "!git clone https://github.com/wayfair/WANDS.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42485468-77e2-4fc3-9a70-319a70603472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define functions for product search using Tf-IDF\n",
    "def calculate_tfidf(dataframe):\n",
    "    \"\"\"\n",
    "    Calculate the TF-IDF for combined product name and description.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): DataFrame with product_id, and other product information.\n",
    "\n",
    "    Returns:\n",
    "    TfidfVectorizer, csr_matrix: TF-IDF vectorizer and TF-IDF matrix.\n",
    "    \"\"\"\n",
    "    # Combine product name and description to vectorize\n",
    "    # NOTE: Please feel free to use any combination of columns available, some columns may contain NULL values\n",
    "    combined_text = dataframe['product_name'] + ' ' + dataframe['product_description']\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # convert combined_text to list of unicode strings\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_text.values.astype('U'))\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def get_top_products(vectorizer, tfidf_matrix, query, top_n=10):\n",
    "    \"\"\"\n",
    "    Get top N products for a given query based on TF-IDF similarity.\n",
    "\n",
    "    Parameters:\n",
    "    vectorizer (TfidfVectorizer): Trained TF-IDF vectorizer.\n",
    "    tfidf_matrix (csr_matrix): TF-IDF matrix for the products.\n",
    "    query (str): Search query.\n",
    "    top_n (int): Number of top products to return.\n",
    "\n",
    "    Returns:\n",
    "    list: List of top N product IDs.\n",
    "    \"\"\"\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    top_product_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    return top_product_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd06913-4149-44c7-a876-787316e1b1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define functions for evaluating retrieval performance\n",
    "def map_at_k(true_ids, predicted_ids, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Average Precision at K (MAP@K).\n",
    "\n",
    "    Parameters:\n",
    "    true_ids (list): List of relevant product IDs.\n",
    "    predicted_ids (list): List of predicted product IDs.\n",
    "    k (int): Number of top elements to consider.\n",
    "             NOTE: IF you wish to change top k, please provide a justification for choosing the new value\n",
    "\n",
    "    Returns:\n",
    "    float: MAP@K score.\n",
    "    \"\"\"\n",
    "    #if either list is empty, return 0\n",
    "    if not len(true_ids) or not len(predicted_ids):\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p_id in enumerate(predicted_ids[:k]):\n",
    "        if p_id in true_ids and p_id not in predicted_ids[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    return score / min(len(true_ids), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc51ce2-521c-428c-8475-57fbc7145d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please add any new evaluation functions here\n",
    "def weighted_map_at_k(\n",
    "        exact_match_ids, partial_match_ids, retrieval_ids, k=10, only_exact_match: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the Weighted Mean Average Precision at K (Weighted MAP@K).\n",
    "\n",
    "    Parameters:\n",
    "    exact_match_ids (list): List of IDs of exact matches.\n",
    "    partial_match_ids (list): List of IDs of partial matches.\n",
    "    retrieval_ids (list): List of retrieved IDs, ordered by rank.\n",
    "    k (int): Number of top elements to consider.\n",
    "    exact_match_weight (float): Importance weight of an exact match.\n",
    "    partial_match_weight (float): Importance weight of a partial match.\n",
    "\n",
    "    Returns:\n",
    "    float: Weighted MAP@K score.\n",
    "    \"\"\"\n",
    "    if not retrieval_ids:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "    weighted_num_hits = 0.0\n",
    "    exact_match_weight = 1.0\n",
    "    partial_match_weight = 0.5\n",
    "    \n",
    "    if only_exact_match:\n",
    "        partial_match_weight = 0.0\n",
    "\n",
    "    seen_ids = set()\n",
    "\n",
    "    for i, retrieved_id in enumerate(retrieval_ids[:k]):\n",
    "        weight = 0.0\n",
    "        if retrieved_id in exact_match_ids:\n",
    "            weight = exact_match_weight\n",
    "        elif retrieved_id in partial_match_ids:\n",
    "            weight = partial_match_weight\n",
    "\n",
    "        if weight > 0 and retrieved_id not in seen_ids:\n",
    "            weighted_num_hits += weight\n",
    "            score += weighted_num_hits / (i + 1.0)\n",
    "            seen_ids.add(retrieved_id)\n",
    "\n",
    "    num_exact_relevant = len(set(exact_match_ids))\n",
    "    num_partial_relevant = len(set(partial_match_ids) - set(exact_match_ids))\n",
    "    total_relevant_weight = (\n",
    "        num_exact_relevant * exact_match_weight + num_partial_relevant * partial_match_weight\n",
    "    )\n",
    "\n",
    "    if total_relevant_weight == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return score / min(total_relevant_weight, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e6f66b3-37d1-48b0-a9f5-fb58003f4cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get search queries\n",
    "query_df = pd.read_csv(\"WANDS/dataset/query.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "041c7b89-1985-4dff-86e6-67353c96bdc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>salon chair</td>\n",
       "      <td>Massage Chairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>smart coffee table</td>\n",
       "      <td>Coffee &amp; Cocktail Tables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dinosaur</td>\n",
       "      <td>Kids Wall Décor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>turquoise pillows</td>\n",
       "      <td>Accent Pillows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>chair and a half recliner</td>\n",
       "      <td>Recliners</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                      query               query_class\n",
       "0         0                salon chair            Massage Chairs\n",
       "1         1         smart coffee table  Coffee & Cocktail Tables\n",
       "2         2                   dinosaur           Kids Wall Décor\n",
       "3         3          turquoise pillows            Accent Pillows\n",
       "4         4  chair and a half recliner                 Recliners"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d55b7820-00e2-4300-966c-8762d15fd407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get products\n",
    "product_df = pd.read_csv(\"WANDS/dataset/product.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0d9c2a5-8c89-41fe-94d4-b0d57a09ca3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_class</th>\n",
       "      <th>category hierarchy</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_features</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>solid wood platform bed</td>\n",
       "      <td>Beds</td>\n",
       "      <td>Furniture / Bedroom Furniture / Beds &amp; Headboa...</td>\n",
       "      <td>good , deep sleep can be quite difficult to ha...</td>\n",
       "      <td>overallwidth-sidetoside:64.7|dsprimaryproducts...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>all-clad 7 qt . slow cooker</td>\n",
       "      <td>Slow Cookers</td>\n",
       "      <td>Kitchen &amp; Tabletop / Small Kitchen Appliances ...</td>\n",
       "      <td>create delicious slow-cooked meals , from tend...</td>\n",
       "      <td>capacityquarts:7|producttype : slow cooker|pro...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>all-clad electrics 6.5 qt . slow cooker</td>\n",
       "      <td>Slow Cookers</td>\n",
       "      <td>Kitchen &amp; Tabletop / Small Kitchen Appliances ...</td>\n",
       "      <td>prepare home-cooked meals on any schedule with...</td>\n",
       "      <td>features : keep warm setting|capacityquarts:6....</td>\n",
       "      <td>208.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>all-clad all professional tools pizza cutter</td>\n",
       "      <td>Slicers, Peelers And Graters</td>\n",
       "      <td>Browse By Brand / All-Clad</td>\n",
       "      <td>this original stainless tool was designed to c...</td>\n",
       "      <td>overallwidth-sidetoside:3.5|warrantylength : l...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>baldwin prestige alcott passage knob with roun...</td>\n",
       "      <td>Door Knobs</td>\n",
       "      <td>Home Improvement / Doors &amp; Door Hardware / Doo...</td>\n",
       "      <td>the hardware has a rich heritage of delivering...</td>\n",
       "      <td>compatibledoorthickness:1.375 '' |countryofori...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                       product_name  \\\n",
       "0           0                            solid wood platform bed   \n",
       "1           1                        all-clad 7 qt . slow cooker   \n",
       "2           2            all-clad electrics 6.5 qt . slow cooker   \n",
       "3           3       all-clad all professional tools pizza cutter   \n",
       "4           4  baldwin prestige alcott passage knob with roun...   \n",
       "\n",
       "                  product_class  \\\n",
       "0                          Beds   \n",
       "1                  Slow Cookers   \n",
       "2                  Slow Cookers   \n",
       "3  Slicers, Peelers And Graters   \n",
       "4                    Door Knobs   \n",
       "\n",
       "                                  category hierarchy  \\\n",
       "0  Furniture / Bedroom Furniture / Beds & Headboa...   \n",
       "1  Kitchen & Tabletop / Small Kitchen Appliances ...   \n",
       "2  Kitchen & Tabletop / Small Kitchen Appliances ...   \n",
       "3                         Browse By Brand / All-Clad   \n",
       "4  Home Improvement / Doors & Door Hardware / Doo...   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  good , deep sleep can be quite difficult to ha...   \n",
       "1  create delicious slow-cooked meals , from tend...   \n",
       "2  prepare home-cooked meals on any schedule with...   \n",
       "3  this original stainless tool was designed to c...   \n",
       "4  the hardware has a rich heritage of delivering...   \n",
       "\n",
       "                                    product_features  rating_count  \\\n",
       "0  overallwidth-sidetoside:64.7|dsprimaryproducts...          15.0   \n",
       "1  capacityquarts:7|producttype : slow cooker|pro...         100.0   \n",
       "2  features : keep warm setting|capacityquarts:6....         208.0   \n",
       "3  overallwidth-sidetoside:3.5|warrantylength : l...          69.0   \n",
       "4  compatibledoorthickness:1.375 '' |countryofori...          70.0   \n",
       "\n",
       "   average_rating  review_count  \n",
       "0             4.5          15.0  \n",
       "1             2.0          98.0  \n",
       "2             3.0         181.0  \n",
       "3             4.5          42.0  \n",
       "4             5.0          42.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4cc385a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42994, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.int64(257)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.int64(4060)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(product_df.shape)\n",
    "display(product_df['product_name'].apply(len).max())\n",
    "display(product_df['product_description'].apply(lambda x: len(x) if isinstance(x, str) else 0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d75f84-1152-43c7-b2a0-422267ab2298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get manually labeled groundtruth lables\n",
    "label_df = pd.read_csv(\"WANDS/dataset/label.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07b8f157-2049-4cdb-afc4-62e546d59dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25434</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12088</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>42931</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2636</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>42923</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  query_id  product_id       label\n",
       "0   0         0       25434       Exact\n",
       "1   1         0       12088  Irrelevant\n",
       "2   2         0       42931       Exact\n",
       "3   3         0        2636       Exact\n",
       "4   4         0       42923       Exact"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb52b884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Exact', 'Irrelevant', 'Partial'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fb9f7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='label', data=label_df)\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb0dbc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exact 0.10972036599156985'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Partial 0.6281184674959734'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Irrelevant 0.26216116651245674'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_counts = label_df['label'].value_counts()\n",
    "size = label_df.shape[0]\n",
    "display(f\"Exact {label_counts['Exact']/size}\")\n",
    "display(f\"Partial {label_counts['Partial']/size}\")\n",
    "display(f\"Irrelevant {label_counts['Irrelevant']/size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d515ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_label_example(label: str):\n",
    "    label_id = label_df[label_df['label'] == label]['id'].iloc[0]\n",
    "    product_id = label_df[label_df['id'] == label_id]['product_id'].iloc[0]\n",
    "    query_id = label_df[label_df['id'] == label_id]['query_id'].iloc[0]\n",
    "    display(label)\n",
    "    display(label_df[label_df['id'] == label_id].head())\n",
    "    display(query_df[query_df['query_id'] == query_id].head())\n",
    "    display(product_df[product_df['product_id'] == product_id].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5fe763b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exact'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25434</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  query_id  product_id  label\n",
       "0   0         0       25434  Exact"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>salon chair</td>\n",
       "      <td>Massage Chairs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id        query     query_class\n",
       "0         0  salon chair  Massage Chairs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_class</th>\n",
       "      <th>category hierarchy</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_features</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25434</th>\n",
       "      <td>25434</td>\n",
       "      <td>21.7 '' w waiting room chair with wood frame</td>\n",
       "      <td>Waiting Room Chairs</td>\n",
       "      <td>Commercial Business Furniture / Commercial Off...</td>\n",
       "      <td>this is a salon chair , barber chair for a hai...</td>\n",
       "      <td>backupholsterycolor : champagne|primarymateria...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id                                  product_name  \\\n",
       "25434       25434  21.7 '' w waiting room chair with wood frame   \n",
       "\n",
       "             product_class                                 category hierarchy  \\\n",
       "25434  Waiting Room Chairs  Commercial Business Furniture / Commercial Off...   \n",
       "\n",
       "                                     product_description  \\\n",
       "25434  this is a salon chair , barber chair for a hai...   \n",
       "\n",
       "                                        product_features  rating_count  \\\n",
       "25434  backupholsterycolor : champagne|primarymateria...           NaN   \n",
       "\n",
       "       average_rating  review_count  \n",
       "25434             NaN           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Irrelevant'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12088</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  query_id  product_id       label\n",
       "1   1         0       12088  Irrelevant"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>salon chair</td>\n",
       "      <td>Massage Chairs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id        query     query_class\n",
       "0         0  salon chair  Massage Chairs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_class</th>\n",
       "      <th>category hierarchy</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_features</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12088</th>\n",
       "      <td>12088</td>\n",
       "      <td>22.5 '' wide polyester side chair</td>\n",
       "      <td>Accent Chairs</td>\n",
       "      <td>Furniture / Living Room Furniture / Chairs &amp; S...</td>\n",
       "      <td>add a beautiful accent to any room with this m...</td>\n",
       "      <td>overalldepth-fronttoback:27.5|design : side ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id                       product_name  product_class  \\\n",
       "12088       12088  22.5 '' wide polyester side chair  Accent Chairs   \n",
       "\n",
       "                                      category hierarchy  \\\n",
       "12088  Furniture / Living Room Furniture / Chairs & S...   \n",
       "\n",
       "                                     product_description  \\\n",
       "12088  add a beautiful accent to any room with this m...   \n",
       "\n",
       "                                        product_features  rating_count  \\\n",
       "12088  overalldepth-fronttoback:27.5|design : side ch...           NaN   \n",
       "\n",
       "       average_rating  review_count  \n",
       "12088             NaN           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Partial'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>4034</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  query_id  product_id    label\n",
       "15  15         0        4034  Partial"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>salon chair</td>\n",
       "      <td>Massage Chairs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id        query     query_class\n",
       "0         0  salon chair  Massage Chairs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_class</th>\n",
       "      <th>category hierarchy</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_features</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4034</th>\n",
       "      <td>4034</td>\n",
       "      <td>aliandra fashion casual lift chair office work...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Furniture / Office Furniture / Office Chairs</td>\n",
       "      <td>applicable scene : office , home life , beauty...</td>\n",
       "      <td>estimatedtimetosetup:15|backcolor : black|over...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id                                       product_name  \\\n",
       "4034        4034  aliandra fashion casual lift chair office work...   \n",
       "\n",
       "     product_class                            category hierarchy  \\\n",
       "4034           NaN  Furniture / Office Furniture / Office Chairs   \n",
       "\n",
       "                                    product_description  \\\n",
       "4034  applicable scene : office , home life , beauty...   \n",
       "\n",
       "                                       product_features  rating_count  \\\n",
       "4034  estimatedtimetosetup:15|backcolor : black|over...           NaN   \n",
       "\n",
       "      average_rating  review_count  \n",
       "4034             NaN           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_label_example('Exact')\n",
    "display_label_example('Irrelevant')\n",
    "display_label_example('Partial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59768978-5c40-45b0-84a7-d6cc5d469b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#group the labels for each query to use when identifying exact matches\n",
    "grouped_label_df = label_df.groupby('query_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1beab6d-1f59-427b-ad9f-1f0c6fcaadcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44307"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(42994, 44307)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate TF-IDF\n",
    "vectorizer, tfidf_matrix = calculate_tfidf(product_df)\n",
    "display(len(vectorizer.vocabulary_))\n",
    "display(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e4b8333-b747-4c3e-87bc-4825f934ab6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top products for 'armchair':\n",
      "12756 24.41 '' wide tufted polyester armchair\n",
      "42698 donham armchair\n",
      "42697 donham 25 '' wide armchair\n",
      "41270 almaraz 33.7 '' wide leather match armchair\n",
      "23907 faizah 27.6 '' wide tufted polyester armchair\n",
      "31564 biloxi 34.75 '' wide armchair\n",
      "41306 hartsell 33 '' wide armchair\n",
      "1527 howington 39 '' wide tufted linen armchair\n",
      "42802 donham polyester lounge chair\n",
      "6532 ogan 29 '' wide polyester armchair\n"
     ]
    }
   ],
   "source": [
    "#Sanity check code block to see if the search results are relevant\n",
    "#implementing a function to retrieve top K product IDs for a query\n",
    "def get_top_product_ids_for_query(query):\n",
    "    top_product_indices = get_top_products(vectorizer, tfidf_matrix, query, top_n=10)\n",
    "    top_product_ids = product_df.iloc[top_product_indices]['product_id'].tolist()\n",
    "    return top_product_ids\n",
    "\n",
    "#define the test query\n",
    "query = \"armchair\"\n",
    "\n",
    "#obtain top product IDs\n",
    "top_product_ids = get_top_product_ids_for_query(query)\n",
    "\n",
    "print(f\"Top products for '{query}':\")\n",
    "for product_id in top_product_ids:\n",
    "    product = product_df.loc[product_df['product_id'] == product_id]\n",
    "    print(product_id, product['product_name'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bdeda61-7fb0-4ca6-a9e1-f1789b539f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#implementing a function to retrieve exact match product IDs for a query_id\n",
    "def get_matches_for_query(query_id: str, label: Literal['Exact', 'Irrelevant', 'Partial']) -> List[int]:\n",
    "    query_group = grouped_label_df.get_group(query_id)\n",
    "    exact_matches = query_group.loc[query_group['label'] == label]['product_id'].values\n",
    "    return exact_matches\n",
    "\n",
    "#applying the function to obtain top product IDs and adding top K product IDs to the dataframe \n",
    "query_df['top_product_ids'] = query_df['query'].apply(get_top_product_ids_for_query)\n",
    "\n",
    "#adding the list of exact match product_IDs from labels_df\n",
    "query_df['exact_match_ids'] = query_df['query_id'].apply(lambda x: get_matches_for_query(x, 'Exact'))\n",
    "query_df['partial_match_ids'] = query_df['query_id'].apply(lambda x: get_matches_for_query(x, 'Partial'))\n",
    "\n",
    "#assign the weighted_map@k score\n",
    "query_df['weighted_map@k'] = query_df.apply(\n",
    "    lambda x: weighted_map_at_k(\n",
    "        x['exact_match_ids'], x['partial_match_ids'], x['top_product_ids'], k=10,\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "#assign the map@k score\n",
    "query_df['map@k'] = query_df.apply(\n",
    "    lambda x: map_at_k(x['exact_match_ids'], x['top_product_ids'], k=10), axis=1\n",
    ")\n",
    "#query_df['map@k'] = query_df.apply(\n",
    "#    lambda x: weighted_map_at_k(\n",
    "#        x['exact_match_ids'], x['partial_match_ids'], x['top_product_ids'], k=10, \n",
    "#        only_exact_match=True\n",
    "#    ), axis=1\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed01f293-d87b-4ab0-811a-d39140ed5638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.29319624944885364)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the MAP across the entire query set\n",
    "query_df.loc[:, 'map@k'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5115f013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.46229599104599106)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the Weighted MAP across the entire query set\n",
    "query_df.loc[:, 'weighted_map@k'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f200289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def calculate_embedding_matrix(dataframe: pd.DataFrame) -> Tuple[SentenceTransformer, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate the embedding matrix for combined product name and description using SentenceTransformer.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): DataFrame with product information.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Embedding matrix.\n",
    "    \"\"\"\n",
    "    # Combine product name and description to vectorize\n",
    "    combined_text = dataframe['product_name'] + ' ' + dataframe['product_description']\n",
    "    #model = SentenceTransformer('intfloat/multilingual-e5-large-instruct')\n",
    "    model = SentenceTransformer(\"Snowflake/snowflake-arctic-embed-m\")\n",
    "\n",
    "    # Convert combined_text to list of unicode strings\n",
    "    combined_text_list = combined_text.values.astype('U')\n",
    "    \n",
    "    # Encode in batches of 128\n",
    "    batch_size = 16\n",
    "\n",
    "    display(\n",
    "        f\"Start encoding {len(combined_text_list)} products with a batch size of {batch_size}. \"\n",
    "        f\"There are {(len(combined_text_list)//batch_size) + 1} batches.\"\n",
    "    )\n",
    "\n",
    "    embedding_matrix = []\n",
    "    for i in range(0, len(combined_text_list), batch_size):\n",
    "        start_time = time.time()\n",
    "        batch_embeddings = model.encode(\n",
    "            combined_text_list[i:i + batch_size], convert_to_tensor=False\n",
    "        )\n",
    "        embedding_matrix.extend(batch_embeddings)\n",
    "        end_time = time.time()\n",
    "        display(f\"Batch {i // batch_size + 1} took {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return model, np.array(embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dc22db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_products_using_semantic_search(\n",
    "        model: SentenceTransformer, embedding_matrix: np.ndarray, query: str, top_n: int = 10\n",
    "):\n",
    "    #task_description: str = (\n",
    "    #    'Given a search query, retrieve relevant passages that answer the query'\n",
    "    #)\n",
    "    #instructed_query: str = f'Instruct: {task_description}\\nQuery: {query}'\n",
    "    #query_vector = model.encode(\n",
    "    #    [instructed_query], convert_to_tensor=False, normalize_embeddings=True\n",
    "    #)[0]\n",
    "    query_vector = model.encode(\n",
    "        [query], prompt_name=\"query\", convert_to_tensor=False\n",
    "    )[0]\n",
    "\n",
    "    cosine_similarities = cosine_similarity(query_vector, embedding_matrix).flatten()\n",
    "    top_product_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    return top_product_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "202975da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desposito/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Start encoding 42994 products with a batch size of 16. There are 2688 batches.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 1 took 7.14 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 2 took 5.68 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 3 took 1.73 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 4 took 4.22 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 5 took 3.48 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 6 took 7.11 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 7 took 3.11 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 8 took 2.97 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 9 took 4.99 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 10 took 2.82 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 11 took 4.93 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 12 took 2.85 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 13 took 3.79 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 14 took 8.84 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 15 took 8.59 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 16 took 4.25 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 17 took 4.55 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 18 took 2.90 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Batch 19 took 3.16 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedding_model, embedding_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_embedding_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m top_product_indices \u001b[38;5;241m=\u001b[39m query_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: get_top_products_using_semantic_search(embedding_model, embedding_matrix, x, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m query_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemantic_search_top_product_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m product_df\u001b[38;5;241m.\u001b[39miloc[top_product_indices][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "Cell \u001b[0;32mIn[25], line 33\u001b[0m, in \u001b[0;36mcalculate_embedding_matrix\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(combined_text_list), batch_size):\n\u001b[1;32m     32\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 33\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombined_text_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     embedding_matrix\u001b[38;5;241m.\u001b[39mextend(batch_embeddings)\n\u001b[1;32m     37\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:623\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 623\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    625\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    437\u001b[0m     key: value\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    440\u001b[0m }\n\u001b[0;32m--> 442\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:409\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(current_states))\n\u001b[0;32m--> 409\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n\u001b[1;32m    411\u001b[0m         key_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m0\u001b[39m], key_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Entrevistas/hbs-assignment/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_model, embedding_matrix = calculate_embedding_matrix(product_df)\n",
    "top_product_indices = query_df['query'].apply(\n",
    "    lambda x: get_top_products_using_semantic_search(embedding_model, embedding_matrix, x, top_n=10)\n",
    ")\n",
    "query_df['semantic_search_top_product_ids'] = product_df.iloc[top_product_indices]['product_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the weighted_map@k score\n",
    "query_df['semantic_weighted_map@k'] = query_df.apply(\n",
    "    lambda x: weighted_map_at_k(\n",
    "        x['exact_match_ids'], x['partial_match_ids'], x['semantic_search_top_product_ids'], k=10,\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "#assign the map@k score\n",
    "query_df['semantic_map@k'] = query_df.apply(\n",
    "    lambda x: map_at_k(x['exact_match_ids'], x['semantic_search_top_product_ids'], k=10), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db71c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(query_df.loc[:, 'semantic_map@k'].mean())\n",
    "display(query_df.loc[:, 'semantic_weighted_map@k'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033664d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Combine searches , re-rank them and calculate map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
